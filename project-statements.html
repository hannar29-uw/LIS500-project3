<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta-name="viewport" content="width=device-width, initial-scale=1">
    <title>LIS 500 Project Statements</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

<!-- HEADER + NAV -->
<header>
    <h1>LIS 500 Project 3</h1>
    <div class="topnav index-text">
        <a href="index.html">Home</a>
        <a href="ml-project.html">Project Scope</a>
        <a href="process.html">Process</a>
        <a href="lessons.html">Lessons Learned</a>
        <a href="video-test.html">Video & Test Model</a>
        <a class="active" href="project-statements.html">Project Statements</a>
    </div>
</header>

<!-- INTRODUCTION -->
<section class="page-section">
    <h2>Our Personal Project Statements</h2>
    <p>
        For this assignment, each member of our team created their own personal project statement
        reflecting on the experience of building our Teachable Machine model and connecting it
        to ideas from <em>Unmasking AI</em>. Writing individual statements let us explain our own
        perspectives, our roles in the project, and the lessons we took from experimenting with a
        machine learning model that tries to identify skin tone through a webcam.
    </p>
</section>

<!-- PROJECT STATEMENTS DROPDOWNS -->
<section class="page-section">

    <h2>Individual Statements</h2>

    <details>
        <summary>Casey – Project Statement</summary>
        <p>
            In working on this project, I was reminded again and again of the limits of techno-utopianism. 
            In the <i>Coded Bias</i> movie, Dr. Buolamwini admits to being drawn to tech because it seemed 
            somehow separate from the problems of the real world: neutral, fair. But what she found through 
            her research was that the problems of the real world were inevitably reflected in technology she studied. 
            This has been my personal experience with technology generally, and with the specific technology 
            we worked with in this project. There is promise, these tools can and do help us solve problems, 
            but the historical and contemporary injustices and outrages of our analog world are repeated in the digital.
        </p>
        <p>
            For our project, we trained Google’s <a href="https://teachablemachine.withgoogle.com/">Teachable-Machines</a> 
            to detect skin-tone from dark to light. What we found was that the <a href= "https://www.tensorflow.org/js">TensorFlow</a> 
            model produced from our training data had difficulty reliably detecting skin-tone. We experimented with different data-sets, 
            and tried focusing on specific areas of the screen for testing, with no real progress. From this I learned two things: 
            one, training a machine-learning model is a vast and complicated process that resists deviation from the guardrails 
            set-up by simplified, tutorial-based models like TeachableMachines; and two, the resources and time it would take 
            to fully learn this technology are prohibitive for the average person, and some AI processes are <a href="https://www.ibm.com/think/topics/black-box-ai">opaque even to experts</a>.     
        </p>
        <p>
            In our extra-credit module, we learned about the concept of a black-box, a hidden operator whose function is 
            unavailable for scrutiny by general users or observers of a device. In our experiments with TeachableMachines, 
            we confronted this concept directly. How does the TeachableMachine model work? Why is it failing with the 
            data-set we are providing it? If we had access to its inner-workings and could understand it more fully, 
            would we be able to modify it or our training data to produce a better outcome? There is potentially a more 
            basic barrier: every step of the process from setting up the GitHub repository to coding on p5js involved 
            endlessly looking up tutorials, googling errors, looking on message boards for advice, and so on. In some ways 
            this was engaging, I felt I was learning a little more about these systems, about how the internet was built, 
            but I couldn’t help feeling that all of this technology was designed and built by a relatively small subset of 
            the human population, with a specific narrow culture and learning style, and thus presents increased difficulty 
            for anyone outside of this culture. 
        </p>
        <p>
            The costs of being outside of this culture range from mild annoyance, as with our experience training TeachableMachines, 
            to mortal danger, as explored by Buolamwini in both the <i>Coded Bias</i> movie, and in her book <i>Unmasking AI</i>. 
            We learn through her book and research, that facial recognition is most effective for white males (the cultural in-group), 
            and error prone for women, people of color, and women of color in particular. This has dire implications as law-enforcement 
            around the world adopt these technologies to track and apprehend people suspected of crimes. Immigration and Customs Enforcement 
            agents in the United States recently began using smart-phone apps to scan the faces of people 
            agents <a href="https://www.npr.org/2025/11/08/nx-s1-5585691/ice-facial-recognition-immigration-tracking-spyware">suspect of being in the country illegally</a>. 
            Little is known by the public about the specific technology being used by the agents, and agents seem to be completely 
            unaware of the potential for it to make mistakes. This means that a person suspected of being in the country illegally 
            because of the color of their skin or the language they are speaking may have their face scanned, without consent, and 
            on the the results of a potentially mistaken scan, may be detained or deported.
        </p>
        <p>
            There is reason for hope. Buolamwini’s popular book, testimony to congress, and success in founding 
            the <i>Algorithmic Justice League</i>, all point to an abiding interest among the human population at large in 
            ideas of fairness and equality. People want to do the right thing, want to create technology that helps instead of hurts. 
            Google’s own initial motto was <a href="https://en.wikipedia.org/wiki/Don%27t_be_evil">“don’t be evil.”</a> 
            The potential for technology to do good is often is scuttled by the conflicting incentives of profit, and for 
            publicly traded companies, appeasing shareholders by maintaining <a href="https://dothemath.ucsd.edu/2022/09/death-by-hockey-sticks/">“hockey-stick”</a> growth. 
            For me, one solution to the problem of bias, and the problem of the black-box, is obvious: human oversight. 
            Buolamwini’s book is an excellent example of this. When she presented her findings of bias in facial recognition 
            software to tech companies, some reacted defensively, but others incorporated her ideas and re-trained models. 
            These companies, IBM in particular, incorporated Buolamwini’s human oversight to correct mistakes. 
            At <a href="https://home.cern/">CERN</a>, the legendary inter-governmental particle physics lab, 
            this oversight is directly addressed in their <a href="https://home.web.cern.ch/news/official-news/knowledge-sharing/general-principles-use-ai-cern">General Principles for the Use of AI</a>. 
            Principle number seven states that “the use of AI must always remain under human control. Its functioning 
            and outputs must be consistently and critically assessed and validated by a human.” 
        </p>
        <p>
           This, to me, is key. In my own experiments with AI, and in our work in class, the most useful outputs 
            have been the ones that I am able to independently verify using outside sources. In the case of our 
            TeachableMachines model, the independent validation is my own human perception, which is much more 
            intrinsically adept at detecting skin-tone than the model we trained. I’m certain that if we had more 
            experience and more time, we could make something that worked better, but I’m also now fully aware that 
            though these machines may be opaque to us in their inner workings, they were made by humans. We’ve learned 
            in this course, in multiple modules, that data-sets are vitally important. What the model is trained on greatly 
            influences is outputs. Given this, the tremendous amount of hidden labor involved in correcting output errors, 
            and the fact that these models were developed by people and prone to all of the biases and blindspots 
            in the culture of their creators, AI seems pretty human after all. 
        </p>
        
    </details>

    <details>
        <summary>Alyssa – Project Statement</summary>
        <p>
            For this project I spent a lot of time thinking about what it means to build something
            that tries to classify people in any way. Working through this assignment after reading
            Buolamwini’s book made me more aware of how much is happening under the surface of even
            the simplest machine learning model. Earlier in the semester we used GitHub while building
            our Project 2 website, so coming into this assignment I already felt a little more confident.
            This project pushed that further because we were not only keeping the same style guide and
            structure from before, we were also refining how we collaborated as a group. We planned our
            pages more intentionally and divided the work in a way that let each of us focus on something
            meaningful while still keeping everything consistent across the site.

        <p>
            One of the biggest pieces of this assignment was deciding what our algorithm would actually do.
            We talked a lot about the experiences Buolamwini describes and how facial analysis systems
            repeatedly fail on darker skin tones, or misread faces entirely. That helped us decide that
            instead of a face recognition model or something complicated, we wanted to build a simple
            Teachable Machine model that tries to read skin tone through a user’s webcam. It felt like a
            way to explore the issues she wrote about without pretending we could solve them. We were more
            interested in seeing how the model behaved and what that would tell us about the limits built
            into this kind of tool.
           
         <p>
            As we trained our classes we realized quickly that the model was not identifying anything in a
            smooth or accurate way. Even when we provided different lighting and angles, it still leaned
            toward misclassifying skin tones. It often labeled things as “dark” even when the person’s skin
            tone was lighter or medium. Sometimes it ignored the person entirely and picked up the background.
            Seeing those mistakes reminded me of how biased these systems are right from the start. The model
            is supposed to reflect whatever data you give it, but even when we tried to be intentional, we saw
            the same issues Buolamwini described. It reinforced the idea that the bias is not only in the data.
            It is built into the way these systems work.
       
        <p>
            This experience connected directly to some of the moments in the book where she talks about feeling
            unseen by the systems she was testing. I started thinking about how frustrating it would be to depend
            on a tool like this in a real situation. Ours was just a simple classroom project and even here it
            struggled to make sense of something as basic as skin tone. The bigger picture is that people are
            judged by these systems all the time, and reading her story made that feel more real while we were
            building our own small version.

         <p>
            Another part of the project that stood out to me was learning more about how to structure a GitHub
            repository in a way that actually supports a collaborative workflow. Last time it felt like we were
            learning as we went. This time we already knew the basic setup so we could focus on the
            actual organization of the files and refining the website. It made collaborating easier and it showed
            me how helpful version control can be when you have several people editing the same project.

         <p>
            Overall, working on this assignment while thinking about Buolamwini’s lessons made the whole process
            feel more intentional. It was not just a project about building a model. It was also about
            understanding how these systems carry bias, even when the people building them are trying to be
            careful. Seeing our own model misread us proved her point in a real and honestly uncomfortable way.

         <p>
            More than anything, I walked away with a better understanding of how technical work and ethical
            thinking need to happen at the same time. You cannot build something first and then worry about
            the impact later. Even a small classroom project can reveal the problems that show up in larger
            systems. This assignment helped me see that clearly.
        </p>
    </details>

    <details>
        <summary>Aymen – Project Statement</summary>
        <p>
            <!-- Paste Aymen's 1000-word statement here -->
            “Unmasking AI,” a book by Dr. Joy Buolamwini, aims to uncover the truths of encoded
            discrimination and exclusion in technology. This is not specifically about content that we 
            are able to see on the user interface; but rather, what is happening behind the scenes that
            fosters discrimination in technology. In a digital world that is being increasingly overrun
            by AI tools and softwares, it is also increasingly important that we understand the functions
            behind these tools, and the shortcomings that come with them. Thus, our group set out to create
            an AI software that can further examine the effect of encoded bias in technology. 
            <p>
            In Unmasking AI, Dr. Buolamwini used a facial recognition software, and encountered interesting 
            results. She noticed that the software would not normally detect her face; 
            however, upon wearing a white mask, the software now detected a face.
            While the purpose of Artificial Intelligence is to be able to sort of act on its own, 
            the thoughts and ideas that it creates are not made up like they are in a human mind. 
            Instead, AI softwares utilize pre-existing information, whether that be datasets, real-time
            data feed, web scraping of the internet, and more. For example, softwares such as 
            Google Gemini utilize web scraping to produce fast responses to user prompts. What did 
            this mean for the software in Buolamwini? Well, the facial recognition software was
            noticeably biased towards white faces. Buolamwini attributed this to an encoded bias
            within the software that is a result of a biased dataset that the software was trained on.
            This software is an example of an image classifying model, which scans visual images and 
            classifies them based on similar images in its pre-existing dataset. For image classifying 
            models such as facial recognition softwares, images of faces are commonly used as references
            for the model. Thus, the inability of the model to detect Dr. Buolamwini’s darker skinned face 
            can be attributed to a lack of darker-skinned data being used to make the model, and a dominance
            of lighter-skinned image data. This is unfortunately a common issue in technology of today, as 
            lighter skinned males are more likely to be represented and served by new technologies, whether 
            they utilize Artificial Intelligence or not.
            <p>
            For this project, our group wanted to perform a similar test and witness the results firsthand. 
            So, we decided to use Teachable Machines to create an image classifying model that scans
            faces and classifies them into a skin tone category. Image classifying models on Teachable
            Machines are made using data (in this case, images) separated into categories. I created 
            three categories: light, medium, and dark skinned. Next, I had people of various skin tones 
            sit in front of the computer camera and scan their face, whilst moving their face around to
            capture all angles. These trials were done in similar lightings and settings. Then, the
            Teachable Machines program created an image classifying model based on these images. 
            <p>
            As a Middle Eastern person, people of my ethnicity and skin tone are often labeled as
            “brown.” So, I was expecting to be detected as medium skinned by the model. For reference,
            not only does the model detect a skin tone, but also the percentage of each skin tone
            detected in the case that the person is not exactly one tone (for example, a person could 
            be detected as 100% dark, 75% medium and 25% light, etc.). To my surprise, my face was 
            detected as 100% dark skinned. I tried adjusting my background lighting, but got the same
            result. However, the palm of my hand was detected as about 90% medium skinned. I had a few
            people test the program, and they were almost always detected as nearly 100% dark skinned.
            I decided to try remaking the model with slightly different data, which was done by removing
            data from the dark skinned category and scanning a different person into the category. 
            I also noticed that one category had significantly more face images than the others, so I
            confined each category to about 130 images. After regenerating the model, I was still
            getting the same results, and after a few lighter-skinned people tested the model, they
            told me that they were also being detected as dark-skinned.
            <p>
            But why is this happening? After all, I reduced encoded biases as much as possible by 
            having the same amount of image samples for each category, having the people being used
            for the samples perform the same movements, and more. Well, the results that I observed 
            are a prime example of the encoded bias that Dr. Buolamwini discussed in Unmasking AI. 
            Artificial Intelligence models are never 100% accurate or reliable, as they feature biases
            that are results of misrepresentation of certain groups in their training data. In cases
            like this, in which the AI model is used on real human beings, we see that these digital 
            shortcomings can have serious effects on underrepresented groups. In Unmasking AI, 
            Dr. Buolamwini discussed how many facial recognition technologies are biased against
            darker-skinned individuals and women. For example, many security cameras utilize facial 
            recognition technology, and in cases such as theft, the facial recognition technology 
            can falsely detect people of underrepresented groups as suspects.
            <p>
            So, how do we minimize encoded bias and discrimination in the technological space? 
            Well, in technologies such as Artificial Intelligence, it is important that we give 
            the AI model sufficient and diverse data to train itself on. For example, when I made 
            the image classifying model on Teachable Machines, maybe I could have scanned faces of 
            more people; or maybe I could have had everyone sit in the same spot in the same room,
            in order to ensure that background lighting and conditions were consistent. Maybe I could
            have included a mix of men and women, in order to capture various facial features that may
            have had an effect on the results of the model. Whatever could have been done, it is important 
            that the data that a model is trained on is representative and equal; representative of 
            various groups, and containing equal conditions during data creation.
        </p>
    </details>

</section>

<!-- FOOTER -->
<footer class="index-text">
    <p>© 2025 by Aymen Hamdan, Alyssa Hannam, Casey Foubert</p>
</footer>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta-name="viewport" content="width=device-width, initial-scale=1">
    <title>LIS 500 Project Statements</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

<!-- HEADER + NAV -->
<header>
    <h1>LIS 500 Project 3</h1>
    <div class="topnav index-text">
        <a href="index.html">Home</a>
        <a href="ml-project.html">Project Scope</a>
        <a href="process.html">Process</a>
        <a href="lessons.html">Lessons Learned</a>
        <a href="video-test.html">Video & Test Model</a>
        <a class="active" href="project-statements.html">Project Statements</a>
    </div>
</header>

<!-- INTRODUCTION -->
<section class="page-section">
    <h2>Our Personal Project Statements</h2>
    <p>
        For this assignment, each member of our team created their own personal project statement
        reflecting on the experience of building our Teachable Machine model and connecting it
        to ideas from <em>Unmasking AI</em>. Writing individual statements let us explain our own
        perspectives, our roles in the project, and the lessons we took from experimenting with a
        machine learning model that tries to identify skin tone through a webcam.
    </p>
</section>

<!-- PROJECT STATEMENTS DROPDOWNS -->
<section class="page-section">

    <h2>Individual Statements</h2>

    <details>
        <summary>Casey – Project Statement</summary>
        <p>
In working on this project, I was reminded again and again of the limits of techno-utopianism. In the <i>Coded Bias</i> movie, Dr. Buolamwini admits to being drawn to tech because it seemed somehow separate from the problems of the real world: neutral, fair. But what she found through her research was that the problems of the real world were inevitably reflected in technology she studied. This has been my personal experience with technology generally, and with the specific technology we worked with in this project. There is promise, these tools can and do help us solve problems, but the historical and contemporary injustices and outrages of our analog world are repeated in the digital.
        </p>
        <p>
    For our project, we trained Google’s <a href="https://teachablemachine.withgoogle.com/">Teachable-Machines</a> to detect skin-tone from dark to light. What we found was that the <a href= "https://www.tensorflow.org/js">TensorFlow</a> model produced from our training data had difficulty reliably detecting skin-tone and almost always defaulted to “dark.” We experimented with different data-sets, and tried focusing on specific areas of the screen for testing, with no real progress. From this I learned two things: one, training a machine-learning model is a vast and complicated process that resists deviation from the guardrails set-up by simplified, tutorial-based models like TeachableMachines; and two, the resources and time it would take to fully learn this technology are prohibitive for the average person, and some AI processes are <a href="https://www.ibm.com/think/topics/black-box-ai">opaque even to experts</a>.     
        </p>
        <p>
            In our extra-credit module, we learned about the concept of a black-box, a hidden operator whose function is unavailable for scrutiny by general users or observers of a device. In our experiments with TeachableMachines, we confronted this concept directly. How does the TeachableMachine model work? Why is it failing with the data-set we are providing it? If we had access to its inner-workings and could understand it more fully, would we be able to modify it or our training data to produce a better outcome? There is potentially a more basic barrier: every step of the process from setting up the GitHub repository to coding on p5js involved endlessly looking up tutorials, googling errors, looking on message boards for advice, and so on. In some ways this was engaging, I felt I was learning a little more about these systems, about how the internet was built, but I couldn’t help feeling that all of this technology was designed and built by a relatively small subset of the human population, with a specific narrow culture and learning style, and thus presents increased difficulty for anyone outside of this culture. 
        </p>
        <p>
            The costs of being outside of this culture range from mild annoyance, as with our experience training TeachableMachines, to mortal danger, as explored by Buolamwini in both the <i>Coded Bias</i> movie, and in her book <i>Unmasking AI</i>. We learn through her book and research, that facial recognition is most effective for white males (the cultural in-group), and error prone for women, people of color, and women of color in particular. This has dire implications as law-enforcement around the world adopt these technologies to track and apprehend people suspected of crimes. Immigration and Customs Enforcement agents in the United States recently began using smart-phone apps to scan the faces of people agents <a href="https://www.npr.org/2025/11/08/nx-s1-5585691/ice-facial-recognition-immigration-tracking-spyware">suspect of being in the country illegally</a>. Little is known by the public about the specific technology being used by the agents, and agents seem to be completely unaware of the potential for it to make mistakes. This means that a person suspected of being in the country illegally because of the color of their skin or the language they are speaking may have their face scanned, without consent, and on the the results of a potentially mistaken scan, may be detained or deported.
        </p>
        <p>
            There is reason for hope. Buolamwini’s popular book, testimony to congress, and success in founding the <i>Algorithmic Justice League</i>, all point to an abiding interest among the human population at large in ideas of fairness and equality. People want to do the right thing, want to create technology that helps instead of hurts. Google’s own initial motto was <a href="https://en.wikipedia.org/wiki/Don%27t_be_evil">“don’t be evil.”</a> The potential for technology to do good is often is scuttled by the conflicting incentives of profit, and for publicly traded companies, appeasing shareholders by maintaining <a href="https://dothemath.ucsd.edu/2022/09/death-by-hockey-sticks/">“hockey-stick”</a> growth. For me, one solution to the problem of bias, and the problem of the black-box, is obvious: human oversight. Buolamwini’s book is an excellent example of this. When she presented her findings of bias in facial recognition software to tech companies, some reacted defensively, but others incorporated her ideas and re-trained models. These companies, IBM in particular, incorporated Buolamwini’s human oversight to correct mistakes. At <a href="https://home.cern/">CERN</a>, the legendary inter-governmental particle physics lab, this oversight is directly addressed in their <a href="https://home.web.cern.ch/news/official-news/knowledge-sharing/general-principles-use-ai-cern">General Principles for the Use of AI</a>. Principle number seven states that “the use of AI must always remain under human control. Its functioning and outputs must be consistently and critically assessed and validated by a human.” 
        </p>
        <p>
           This, to me, is key. In my own experiments with AI, and in our work in class, the most useful outputs have been the ones that I am able to independently verify using outside sources. In the case of our TeachableMachines model, the independent validation is my own human perception, which is much more intrinsically adept at detecting skin-tone than the model we trained. I’m certain that if we had more experience and more time, we could make something that worked better, but I’m also now fully aware that though these machines may be opaque to us in their inner workings, they were made by humans. We’ve learned in this course, in multiple modules, that data-sets are vitally important. What the model is trained on greatly influences is outputs. Given this, the tremendous amount of hidden labor involved in correcting output errors, and the fact that these models were developed by people and prone to all of the biases and blindspots in the culture of their creators, AI seems pretty human after all. 
        </p>
        
    </details>

    <details>
        <summary>Alyssa – Project Statement</summary>
        <p>
            For this project I spent a lot of time thinking about what it means to build something
            that tries to classify people in any way. Working through this assignment after reading
            Buolamwini’s book made me more aware of how much is happening under the surface of even
            the simplest machine learning model. Earlier in the semester we used GitHub while building
            our Project 2 website, so coming into this assignment I already felt a little more confident.
            This project pushed that further because we were not only keeping the same style guide and
            structure from before, we were also refining how we collaborated as a group. We planned our
            pages more intentionally and divided the work in a way that let each of us focus on something
            meaningful while still keeping everything consistent across the site.

        <p>
            One of the biggest pieces of this assignment was deciding what our algorithm would actually do.
            We talked a lot about the experiences Buolamwini describes and how facial analysis systems
            repeatedly fail on darker skin tones, or misread faces entirely. That helped us decide that
            instead of a face recognition model or something complicated, we wanted to build a simple
            Teachable Machine model that tries to read skin tone through a user’s webcam. It felt like a
            way to explore the issues she wrote about without pretending we could solve them. We were more
            interested in seeing how the model behaved and what that would tell us about the limits built
            into this kind of tool.
           
         <p>
            As we trained our classes we realized quickly that the model was not identifying anything in a
            smooth or accurate way. Even when we provided different lighting and angles, it still leaned
            toward misclassifying skin tones. It often labeled things as “dark” even when the person’s skin
            tone was lighter or medium. Sometimes it ignored the person entirely and picked up the background.
            Seeing those mistakes reminded me of how biased these systems are right from the start. The model
            is supposed to reflect whatever data you give it, but even when we tried to be intentional, we saw
            the same issues Buolamwini described. It reinforced the idea that the bias is not only in the data.
            It is built into the way these systems work.
       
        <p>
            This experience connected directly to some of the moments in the book where she talks about feeling
            unseen by the systems she was testing. I started thinking about how frustrating it would be to depend
            on a tool like this in a real situation. Ours was just a simple classroom project and even here it
            struggled to make sense of something as basic as skin tone. The bigger picture is that people are
            judged by these systems all the time, and reading her story made that feel more real while we were
            building our own small version.

         <p>
            Another part of the project that stood out to me was learning more about how to structure a GitHub
            repository in a way that actually supports a collaborative workflow. Last time it felt like we were
            learning as we went. This time we already knew the basic setup so we could focus on the
            actual organization of the files and refining the website. It made collaborating easier and it showed
            me how helpful version control can be when you have several people editing the same project.

         <p>
            Overall, working on this assignment while thinking about Buolamwini’s lessons made the whole process
            feel more intentional. It was not just a project about building a model. It was also about
            understanding how these systems carry bias, even when the people building them are trying to be
            careful. Seeing our own model misread us proved her point in a real and honestly uncomfortable way.

         <p>
            More than anything, I walked away with a better understanding of how technical work and ethical
            thinking need to happen at the same time. You cannot build something first and then worry about
            the impact later. Even a small classroom project can reveal the problems that show up in larger
            systems. This assignment helped me see that clearly.
        </p>
    </details>

    <details>
        <summary>Aymen – Project Statement</summary>
        <p>
            <!-- Paste Aymen's 1000-word statement here -->
            REPLACE WITH AYMEN'S PROJECT STATEMENT
        </p>
    </details>

</section>

<!-- FOOTER -->
<footer class="index-text">
    <p>© 2025 by Aymen Hamdan, Alyssa Hannam, Casey Foubert</p>
</footer>

</body>
</html>
